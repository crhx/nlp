{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MusicBuild.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNsoICuySdFJX72hFwZZ3MA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYEP-kWK1WbU",
        "outputId": "1a263040-547f-4f11-e4ff-e8cc21c7dbba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MIDI-BERT'...\n",
            "remote: Enumerating objects: 774, done.\u001b[K\n",
            "remote: Counting objects: 100% (167/167), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 774 (delta 78), reused 81 (delta 32), pack-reused 607\u001b[K\n",
            "Receiving objects: 100% (774/774), 75.34 MiB | 7.86 MiB/s, done.\n",
            "Resolving deltas: 100% (392/392), done.\n",
            "Checking out files: 100% (153/153), done.\n",
            "/content/MIDI-BERT/MidiBERT/CP/MIDI-BERT/MidiBERT/CP/MIDI-BERT/MidiBERT/CP/MIDI-BERT\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.21.5)\n",
            "Requirement already satisfied: matplotlib>=3.3.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.5.1)\n",
            "Requirement already satisfied: mido==1.2.10 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.2.10)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.9.0)\n",
            "Requirement already satisfied: chorder==0.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.1.2)\n",
            "Requirement already satisfied: miditoolkit==0.1.14 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.1.14)\n",
            "Requirement already satisfied: scikit_learn==0.24.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (0.24.2)\n",
            "Requirement already satisfied: torchaudio==0.9.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: transformers==4.8.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (4.8.2)\n",
            "Requirement already satisfied: SoundFile in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.10.3.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (4.62.3)\n",
            "Requirement already satisfied: pypianoroll in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.0.4)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2->-r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2->-r requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->-r requirements.txt (line 4)) (3.10.0.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 9)) (4.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 9)) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 9)) (0.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 9)) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 9)) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 9)) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 9)) (0.0.47)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 9)) (21.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 9)) (3.13)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.3->-r requirements.txt (line 2)) (3.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.3->-r requirements.txt (line 2)) (4.29.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.3->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.3->-r requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.3->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.3->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.3->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from SoundFile->-r requirements.txt (line 10)) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->SoundFile->-r requirements.txt (line 10)) (2.21)\n",
            "Requirement already satisfied: pretty-midi>=0.2.8 in /usr/local/lib/python3.7/dist-packages (from pypianoroll->-r requirements.txt (line 12)) (0.2.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.8.2->-r requirements.txt (line 9)) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 9)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 9)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 9)) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.2->-r requirements.txt (line 9)) (7.1.2)\n",
            "/content/MIDI-BERT/MidiBERT/CP/MIDI-BERT/MidiBERT/CP/MIDI-BERT/MidiBERT/CP/MIDI-BERT/MidiBERT\n",
            "/content/MIDI-BERT/MidiBERT/CP/MIDI-BERT/MidiBERT/CP/MIDI-BERT/MidiBERT/CP/MIDI-BERT/MidiBERT/CP\n"
          ]
        }
      ],
      "source": [
        "# I followed the instructions on the MIDI-BERT GitHub repository \n",
        "# for setting up the model and loading the weights from the pretraining checkpoint.\n",
        "# Then after investigating the model parameters I converted it \n",
        "# into a sequence classification model compatible with run_glue.py\n",
        "\n",
        "!git clone https://github.com/wazenmai/MIDI-BERT.git\n",
        "%cd MIDI-BERT\n",
        "!pip install -r requirements.txt\n",
        "%cd MidiBERT\n",
        "%cd CP\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/Colab\\ Data/Music/pretrain_model.ckpt ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhKI4FYl-JKQ",
        "outputId": "3e860a5c-e82a-4906-b2c7-f13fa0c1f821"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "import random\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from transformers import BertConfig, BertForSequenceClassification\n",
        "\n",
        "from model import MidiBert\n",
        "from finetune_trainer import FinetuneTrainer\n",
        "from finetune_dataset import FinetuneDataset\n",
        "\n",
        "seed = 2021\n",
        "torch.manual_seed(seed)             # cpu\n",
        "    \n",
        "torch.cuda.manual_seed(seed)        # current gpu\n",
        "torch.cuda.manual_seed_all(seed)    # all gpu\n",
        "random.seed(seed)\n",
        "\n",
        "print(\"Loading Dictionary\")\n",
        "with open('../../dict/CP.pkl', 'rb') as f:\n",
        "    e2w, w2e = pickle.load(f)\n",
        "\n",
        "print(\"\\nBuilding BERT model\")\n",
        "configuration = BertConfig(max_position_embeddings=512,\n",
        "                                position_embedding_type='relative_key_query',\n",
        "                                hidden_size=768)\n",
        "\n",
        "midibert = MidiBert(bertConfig=configuration, e2w=e2w, w2e=w2e)\n",
        "checkpoint = torch.load('pretrain_model.ckpt')#, map_location='cpu')\n",
        "midibert.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNo3I8811gM-",
        "outputId": "57129f8b-3841-4352-ada9-fd39f6cbf5e2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Dictionary\n",
            "\n",
            "Building BERT model\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display model parameters\n",
        "for name, para in midibert.named_parameters():\n",
        "    print('{}: {}'.format(name, para.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3sf2gfeGDsr",
        "outputId": "0a01bbdf-9131-45cf-c247-e30f45290cf0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert.embeddings.word_embeddings.weight: torch.Size([30522, 768])\n",
            "bert.embeddings.position_embeddings.weight: torch.Size([512, 768])\n",
            "bert.embeddings.token_type_embeddings.weight: torch.Size([2, 768])\n",
            "bert.embeddings.LayerNorm.weight: torch.Size([768])\n",
            "bert.embeddings.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.attention.self.distance_embedding.weight: torch.Size([1023, 64])\n",
            "bert.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.0.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.0.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.attention.self.distance_embedding.weight: torch.Size([1023, 64])\n",
            "bert.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.1.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.1.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.attention.self.distance_embedding.weight: torch.Size([1023, 64])\n",
            "bert.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.2.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.2.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.attention.self.distance_embedding.weight: torch.Size([1023, 64])\n",
            "bert.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.3.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.3.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.attention.self.distance_embedding.weight: torch.Size([1023, 64])\n",
            "bert.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.4.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.4.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.attention.self.distance_embedding.weight: torch.Size([1023, 64])\n",
            "bert.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.5.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.5.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.attention.self.distance_embedding.weight: torch.Size([1023, 64])\n",
            "bert.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.6.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.6.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.attention.self.distance_embedding.weight: torch.Size([1023, 64])\n",
            "bert.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.7.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.7.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.attention.self.distance_embedding.weight: torch.Size([1023, 64])\n",
            "bert.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.8.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.8.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.attention.self.distance_embedding.weight: torch.Size([1023, 64])\n",
            "bert.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.9.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.9.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.attention.self.distance_embedding.weight: torch.Size([1023, 64])\n",
            "bert.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.10.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.10.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.attention.self.distance_embedding.weight: torch.Size([1023, 64])\n",
            "bert.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.11.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.11.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.pooler.dense.weight: torch.Size([768, 768])\n",
            "bert.pooler.dense.bias: torch.Size([768])\n",
            "word_emb.0.lut.weight: torch.Size([4, 256])\n",
            "word_emb.1.lut.weight: torch.Size([18, 256])\n",
            "word_emb.2.lut.weight: torch.Size([88, 256])\n",
            "word_emb.3.lut.weight: torch.Size([66, 256])\n",
            "in_linear.weight: torch.Size([768, 1024])\n",
            "in_linear.bias: torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(midibert.state_dict(), \"/oldModel\") # save as pytorch_model.bin\n",
        "\n",
        "# convert to sequence classification model\n",
        "config = BertConfig(max_position_embeddings=512, position_embedding_type='relative_key_query', hidden_size=768)\n",
        "model = BertForSequenceClassification.from_pretrained(\"/oldModel\", config=config)\n",
        "\n",
        "# Display model parameters\n",
        "for name, para in midibert.named_parameters():\n",
        "    print('{}: {}'.format(name, para.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fNKaVylGs-o",
        "outputId": "ed7e1c03-c5cc-41cc-b8b4-a6b615a42791"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /oldModel were not used when initializing BertForSequenceClassification: ['word_emb.1.lut.weight', 'word_emb.2.lut.weight', 'in_linear.weight', 'in_linear.bias', 'word_emb.3.lut.weight', 'word_emb.0.lut.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /oldModel and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# quickly check that the weights transferred properly by comparing the input embedding layer\n",
        "for name, para in model.named_parameters():\n",
        "    print('{}: {}'.format(name, para.shape))\n",
        "    print(para)\n",
        "    break\n",
        "for name, para in midibert.named_parameters():\n",
        "    print('{}: {}'.format(name, para.shape))\n",
        "    print(para)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3zXYm3pMo3P",
        "outputId": "05defad1-a4b0-4572-fe5f-a30fb6326253"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert.embeddings.word_embeddings.weight: torch.Size([30522, 768])\n",
            "Parameter containing:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0026,  0.0093,  0.0173,  ..., -0.0044,  0.0002, -0.0008],\n",
            "        [-0.0297,  0.0087, -0.0348,  ...,  0.0274, -0.0368,  0.0207],\n",
            "        ...,\n",
            "        [ 0.0083, -0.0441,  0.0109,  ...,  0.0121, -0.0516,  0.0148],\n",
            "        [-0.0381,  0.0194,  0.0050,  ...,  0.0062, -0.0129,  0.0059],\n",
            "        [ 0.0405,  0.0071,  0.0143,  ..., -0.0107,  0.0205, -0.0196]],\n",
            "       requires_grad=True)\n",
            "bert.embeddings.word_embeddings.weight: torch.Size([30522, 768])\n",
            "Parameter containing:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0026,  0.0093,  0.0173,  ..., -0.0044,  0.0002, -0.0008],\n",
            "        [-0.0297,  0.0087, -0.0348,  ...,  0.0274, -0.0368,  0.0207],\n",
            "        ...,\n",
            "        [ 0.0083, -0.0441,  0.0109,  ...,  0.0121, -0.0516,  0.0148],\n",
            "        [-0.0381,  0.0194,  0.0050,  ...,  0.0062, -0.0129,  0.0059],\n",
            "        [ 0.0405,  0.0071,  0.0143,  ..., -0.0107,  0.0205, -0.0196]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/newModel\")\n",
        "model.save_pretrained(\"/content/drive/MyDrive/Colab Data/MusicBuild/\")"
      ],
      "metadata": {
        "id": "_HOms86TOuSn"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}